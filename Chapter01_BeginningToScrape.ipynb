{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Your First Web Scraper\n",
    "\n",
    "Browser does a lot for you. (HTML formatting, CSS, Javascript, etc) \n",
    "\n",
    "Let's start with GET request. \n",
    "\n",
    "## Connecting\n",
    "\n",
    "GET request: \n",
    "- Client sends a stream of 1 and 0. (information that contains a header, body. The header contains an immediate destination of local router's MAC with a final destination to server's IP address. The body contains the request.)\n",
    "- Client's local router receives it, interprets them as packet. The router stamps its own IP on the packet as the 'from' IP and sends it off. \n",
    "- Client's packet traverses several intermediary servers to the target server. \n",
    "- The server receives the packet at its IP address. \n",
    "- The server reads the packet port destination in the header and passes it off to the appropriate web server application.\n",
    "- The server application receives a stream of data from server processor. It says:\n",
    "    - This is a GET request. \n",
    "    - The following file is requested: index.html\n",
    "- The web server locates the correct file and bundles it up into a new packet. Then sends it to the client. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "IP address is like the street address, packet port is like an apartment number for packet data. \n",
    "\n",
    "** packet port destination is almost always port 80 for web applications. \n",
    "\n",
    "<br>\n",
    "\n",
    "The above process does not require a browser. Web browser is a recent invention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen('http://pythonscraping.com/pages/page1.html') # not only html but any other file stream. \n",
    "html.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "http.client.HTTPResponse"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accurately, this outputs the HTML \"file\" page1.html, found in the directory < web root >/pages, on the server located at the domain name http://pythonscraping.com\n",
    "\n",
    "Not a page, a file. (It is important to think this way.)\n",
    "\n",
    "urllib is a standard Python library. (batteries included)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to BeautifulSoup \n",
    "\n",
    "Format and organize messy web by \n",
    "- fixing bad HTMl \n",
    "- and presenting us with easily traversable Python objects representing XML structures. \n",
    "\n",
    "### Installing BeautifulSoup\n",
    "\n",
    "pip install beautifulsoup4 (or pipenv install beautifulsoup4. It's NOT beautifulsoup.)\n",
    "\n",
    "** It is always good to have a virtualenv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>A Useful Page</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>An Interesting Title</h1>\n",
       "<div>\n",
       "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser') # HTML content transformed into a BeautifulSoup object. \n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>An Interesting Title</h1>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bs.h1 returns only the first instance of the h1 tag.\n",
    "\n",
    "By convention, only one h1 tag should be used on a single page. (but this convention is often broken in the wild) \n",
    "\n",
    "The below is also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>An Interesting Title</h1>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.html.body.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>An Interesting Title</h1>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.body.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another parser option is lxml. \n",
    "\n",
    "This can be installed by \n",
    "\n",
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs = BeautifulSoup(html.read(), 'lxml')\n",
    "# bs\n",
    "\n",
    "## error. \n",
    "# https://stackoverflow.com/questions/24398302/bs4-featurenotfound-couldnt-find-a-tree-builder-with-the-features-you-requeste\n",
    "# python -m pip install lxml seems to fix the problem but I won't bother."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lxml has some advantage over html.parser:\n",
    "- generally better at parsing 'messy' or malformed HTML. \n",
    "- like unclosed tags or improperly nested. \n",
    "- somewhat faster than html.parser\n",
    "\n",
    "some disadvantages are:\n",
    "- it depends on third-party C libraries to function. \n",
    "- bad portability / ease of use \n",
    "\n",
    "Another option is 'html5lib'. (also good for messier HTML, but slower than the previous two.)\n",
    "\n",
    "### Connecting Reliably and Handling Exceptions \n",
    "\n",
    "Web is messy. (data poorly formatted, website down, closing tags missing, etc)\n",
    "\n",
    "You have to take care of exceptions if you don't want to face error in the next morning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server could not be found!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "# html = urlopen('http://www.pythonscraping.com/pages/page1.html') # This can't handle 404 or 500 HTTP error. \n",
    "\n",
    "try:\n",
    "    html = urlopen(\"https://pythonscrapingthisurldoesnotexist.com\")\n",
    "except HTTPError as e:\n",
    "    print(\"The server returned an HTTP error\") # Any HTTP error. \n",
    "except URLError as e:\n",
    "    print(\"The server could not be found!\") # Site is down or url is mistyped. The server can't even send out HTTP error. (more serious)\n",
    "else:\n",
    "    print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        bsObj = BeautifulSoup(html.read(), \"lxml\")\n",
    "        title = bsObj.body.h1\n",
    "    except AttributeError as e: # 'Is it a Nonetype object? Any possibility of error?' \n",
    "        return None\n",
    "    return title\n",
    "\n",
    "\n",
    "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "if title == None:\n",
    "    print(\"Title could not be found\")\n",
    "else:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing scrapers, it's important to think about the overall pattern of your code. \n",
    "- handle exceptions and make it readable at the same time. \n",
    "- reuse code \n",
    "    - eg: getSiteHTML(), getTitle(), ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
