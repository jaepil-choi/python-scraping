{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Web Crawling Models\n",
    "\n",
    "Although the applications of web crawlers are nearly endless, large scalable crawlers tend to fall into one of several patterns. By learning these patterns and recognizing the situations they apply to, you can vastly improve the maintainability and robustness of your web crawlers.\n",
    "\n",
    "This chapter focuses primarily on web crawlers that collect a limited number of “types” of data (such as restaurant reviews, news articles, company profiles) from a variety of websites, and that store these data types as Python objects that read and write from a database.\n",
    "\n",
    "## Planning and Defining Objects\n",
    "\n",
    "One common trap of web scraping is defining the data that you want to collect based entirely on what’s available in front of your eyes.\n",
    "\n",
    "Simply adding attributes to your product type every time you see a new piece of information on a website will lead to far too many fields to keep track of.\\\n",
    "\n",
    "Solution: Don't scrape every piece of data on every website. Instead, decide which data to collect first. (\"What do I need?\" rather than \"What exists?\")\n",
    "\n",
    "Think about the database efficiency. \n",
    "\n",
    "\n",
    "In this case, you need enough information to uniquely identify the product, and that’s it:\n",
    "- Product title\n",
    "- Manufacturer\n",
    "- Product ID number (if available/relevant)\n",
    "\n",
    "It’s important to note that none of this information is specific to a particular store.\n",
    "\n",
    "Other information (colors the product comes in, what it’s made of) is specific to the product, but may be sparse—it’s not applicable to every product. It’s important to take a step back and perform a checklist for each item you consider and ask yourself the following questions:\n",
    "- Will this information help with the project goals? Will it be a roadblock if I don’t have it, or is it just “nice to have” but won’t ultimately impact anything?\n",
    "- If it might help in the future, but I’m unsure, how difficult will it be to go back and collect the data at a later time?\n",
    "- Is this data redundant to data I’ve already collected?\n",
    "- Does it make logical sense to store the data within this particular object? (As mentioned before, storing a description in a product doesn’t make sense if that description changes from site to site for the same product.)\n",
    "\n",
    "If you do decide that you need to collect the data, it’s important to ask a few more questions to then decide how to store and handle it in code:\n",
    "- Is this data sparse or dense? Will it be relevant and populated in every listing, or just a handful out of the set?\n",
    "- How large is the data?\n",
    "- Especially in the case of large data, will I need to regularly retrieve it every time I run my analysis, or only on occasion?\n",
    "- How variable is this type of data? Will I regularly need to add new attributes, modify types (such as fabric patterns, which may be added frequently), or is it set in stone (shoe sizes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(url):\n",
    "    \"\"\"\n",
    "    Utilty function used to get a Beautiful Soup object from a given URL\n",
    "    \"\"\"\n",
    "\n",
    "    session = requests.Session()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'}\n",
    "    try:\n",
    "        req = session.get(url, headers=headers)\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "    bs = BeautifulSoup(req.text, 'html.parser')\n",
    "    return bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with different website layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Delivering inclusive urban access: 3 uncomfortable truths\n",
      "URL: https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/\n",
      "\n",
      "\n",
      "The past few decades have been filled with a deep optimism about the role of cities and suburbs across the world. These engines of economic growth host a majority of world population, are major drivers of economic innovation, and have created pathways to opportunities for untold amounts of people.\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jeffrey Gutman\n",
      "Nonresident Senior Fellow - Global Economy and Development\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Adie Tomer\n",
      "Fellow - Metropolitan Policy Program\n",
      "\n",
      " Twitter\n",
      "AdieTomer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "But all is not well within our so-called Urban Century. Rapid urbanization, rising gentrification, concentrated poverty, and shortages of basic infrastructure have combined to create spatial inequity in cities and suburbs across the globe. The challenges of housing, moving, and employing so many people have led to longer travel times, rising housing costs, and unsustainable public spending. Moreover, policymakers are questioning traditional policies and approaches.\n",
      "The past couple years, we’ve led a project at Brookings—Moving to Access—that responds to these spatial challenges by promoting the idea of connecting people to opportunities as a new foundational principle for 21st century urban development. This principle of accessibility is meant to be a corollary to the natural questions we ask ourselves everyday about the communities where we live: Is this the best location to access employment? Are there nearby schools and health services? Is there a market in the neighborhood? How can I get from here to there? Such choices are valid for those with sufficient income. But what about those with more limited resources and thus choices in terms of affordable housing and affordable transport?\n",
      "While economists, planners, and engineers have promoted accessibility for decades, the concept is more often found in textbooks than formal urban policies. In the first stage of this project, we worked with a team of experts to determine what has stalled practical implementation of appropriate policies and practices? “Delivering Inclusive Access,” a report of this initial work, offers a synthesis of what we found and where we believe researchers, policymakers, and practitioners can take this work next. The paper found three central challenges.\n",
      "The fallacy of the single indicator\n",
      "The current transport regime’s approach to measurement is one of outward elegance: The dominant pursuit is speed, and the primary way to measure it is congestion (or what slows us down). Many have come to label this approach a pursuit of “mobility.” It is seen through different, but often singular, measures of how congestion affects a specific roadway. Such singular measures are easily interpreted by policymakers and civil society and can be translated directly into economic analysis of related investments through timesavings. They also conveniently serve such purposes as the internationally agreed-upon Sustainable Development Goals. Yet they actually don’t answer the fundamental question of who can reach where, in how much time, and at what cost.\n",
      "\n",
      "\n",
      "Related Content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "Delivering inclusive access\n",
      "\n",
      "Jeffrey Gutman, Adie Tomer, Joseph W. Kane, Nirav Patel, and Ranjitha Shivaram\n",
      "August 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "Measuring performance: Accessibility metrics in metropolitan regions around the world\n",
      "\n",
      "Geneviève Boisjoly and Ahmed El-Geneidy\n",
      "August 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "Is better access key to inclusive cities?\n",
      "\n",
      "Jeffrey Gutman and Nirav Patel\n",
      "Wednesday, October 5, 2016\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accessibility measures can answer those questions, but not through any one measure. First, the variable social, economic, and political contexts related to access mean searching for a single magical indicator is counterintuitive. For example, a wealthy, automobile-centric region like Dallas, Texas, may have very different measurable goals than a denser, poorer region like Dar es Salaam, Tanzania. Second, academic literature is now rife with such complex measures that it could be difficult to communicate their methodology and results with practitioners. The development of a suite of indicators could offer a menu for policymakers and practitioners to judge accessibility based on local objectives, local conditions, and local capacity.\n",
      "The danger of excessive localization\n",
      "Decentralization and empowering local communities is fast becoming a mantra of governance experts across the world, from development practitioners at institutions like the World Bank to city-focused theorists. And for good reason: delegating policy design and fiscal authority directly to the local level helps ensure policies and practices respond to local needs and desires. Yet as urban areas spillover into contiguous and often numerous municipalities, local independence can introduce certain challenges, especially relating to social and environmental externalities. When it comes to transportation and land development, interests of one municipality are often different from its neighbors. And these divergent development goals can exacerbate accessibility challenges within growing regions, spreading people, housing jobs, and other activities further from one another.\n",
      "Addressing spatial inequities in land use and real estate markets require a broader approach to horizontal governance. While there are examples of metropolitan transport authorities, there is less willingness to consider metropolitan or horizontal governance of land use and fiscal policies. For example, should housing be coordinated across an entire region?\n",
      "Countries with a more centralized top down approach to governance, such as France and Germany, have greater ability to formulate metropolitan governance than more decentralized countries such as the U.S. This is not to say there is a one-size-fits-all approach, but there is an opportunity to test different solutions within different governance contexts, comparing how effective each model is to promote spatial inclusivity.\n",
      "The finance community is missing in action\n",
      "Financing is a central topic in infrastructure circles. As maintenance bills from the automobile era come due, populations continue to grow, and fiscal budgets are tight, how can urban areas afford to build enough infrastructure to support future economic growth? In response, new approaches are evolving in fiscal instruments, such as value capture and private-public partnerships. Missing in these discussions, however, are the implications for inclusive access.\n",
      "We conducted a multi-decade review of past academic literature on access and found that there is no clear substantive discussion of accessibility from a fiscal perspective. While urban transport and land use professionals clearly recognize their interrelationship in achieving inclusive accessibility, at least in theory, the fiscal and finance professionals generally ignore the implications of their instruments with regard to inclusivity. The multilateral development banks and their economic evaluations have ignored the distributive impacts until very recently. And the efforts of some countries to incorporate measures through multi-criteria analysis have had limited impact.\n",
      "This gap must be resolved in any effort toward inclusive urban development. There is little doubt that fiscal approaches must carefully assess who ultimately pays and that alternative finance instruments should be adapted to foster access for all.\n",
      "Going forward\n",
      "Our research confirms that there are enormous opportunities to advance accessibility theory into practice. At this point, what is desperately needed is to launch a range of case studies that deal with these issues and challenges under different geographic, governance, and economic contexts. The good news is that many initiatives are already underway, and more robust communication channels and technology can support such efforts. In Chicago, researchers created an online platform to visually explore accessibility by location. In Bogota, researchers evaluated how affordability is a key principle of access. And in Cairo and Kigali, researchers used open tools to achieve new insights for accessibility. Sharing the results of these case studies could lead to a new level of cross-disciplinary approaches to improve accessibility and lessen the effects of spatial inequity.\n",
      "\n",
      "Title: The Men Who Want to Live Forever\n",
      "URL: https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html\n",
      "\n",
      "Would you like to live forever? Some billionaires, already invincible in every other way, have decided that they also deserve not to die. Today several biotech companies, fueled by Silicon Valley fortunes, are devoted to “life extension” — or as some put it, to solving “the problem of death.”\n",
      "It’s a cause championed by the tech billionaire Peter Thiel, the TED Talk darling Aubrey de Gray, Google’s billion-dollar Calico longevity lab and investment by Amazon’s Jeff Bezos. The National Academy of Medicine, an independent group, recently dedicated funding to “end aging forever.”\n",
      "As the longevity entrepreneur Arram Sabeti told The New Yorker: “The proposition that we can live forever is obvious. It doesn’t violate the laws of physics, so we can achieve it.” Of all the slightly creepy aspects to this trend, the strangest is the least noticed: The people publicly championing life extension are mainly men.\n",
      "Not all of them, of course. In 2009, Elizabeth Blackburn received the Nobel Prize for her work on telomeres, protein caps on chromosomes that may be a key to understanding aging. Cynthia Kenyon, the vice president for aging research at Calico, studied life extension long before it was cool; her former protégée, Laura Deming, now runs a venture capital fund for the cause. But these women are focused on curbing age-related pathology, a concept about as controversial as cancer research. They do not appear thirsty for the Fountain of Youth.\n",
      "Professor Blackburn’s new book on telomeres couldn’t be clearer. “Does our research show that by maintaining your telomeres you will live into your hundreds?” it says. “No. Everyone’s cells become old and eventually we die.” Ms. Kenyon once described her research’s goal as “to just have a healthy life and then turn out the lights.” Even Ms. Deming, a 23-year-old prodigy who worked in Ms. Kenyon’s lab at age 12, points out that “aging is innately important to us.”\n",
      "Few of these experts come close to matching the gaudy statements of the longevity investor and “biohacker” Dave Asprey, who has told journalists, “I decided that I was just not going to die.” Or those of Brian Hanley, a microbiologist who has tested an anti-aging gene therapy he developed on himself, who claimed: “There’s a bunch of things that will need to be done to achieve life spans into at least hundreds of years. But we’ll get there.” Or of the 74-year-old fashion mogul Peter Nygard, who during a promotional clip receives injections of his own stem cells to reverse his aging while declaring: “Ponce de León had the right idea. He was just too early. That was then. This is now.”\n",
      "I came across Mr. Nygard’s ode to human endurance three years ago while beginning research on a novel about a woman who can’t die, and watching that video allowed me to experience something close to life extension. As Mr. Nygard compared himself to Leonardo da Vinci and Benjamin Franklin while dancing with a bevy of models — or as a voice-over explained, “living a life most can only dream of” — nine minutes of YouTube expanded into a vapid eternity, where time melted into a vortex of solipsism.\n",
      "At that time I was immersed in caring for my four young children, and this paean to everlasting youth seemed especially stupid. I recall thinking that if this was eternal life, death didn’t seem that bad.\n",
      "But now, as powerful men have begun falling like dominoes under accusations of sexual assault, that video with its young women clustered around an elderly multimillionaire has haunted me anew. As I recall my discomfort with the proclamations of longevity-driven men who hope to achieve “escape velocity,” I think of the astonishing hubris of the Harvey Weinsteins of the world, those who saw young women’s bodies as theirs for the taking.\n",
      "Much has been said about why we allowed such behavior to go unchecked. What has remained unsaid, because it is so obvious, is what would make someone so shameless in the first place: These people believed they were invincible. They saw their own bodies as entirely theirs and other people’s bodies as at their disposal; apparently nothing in their lives led them to believe otherwise.\n",
      "Historically, this is a mistake that few women would make, because until very recently, the physical experience of being a woman entailed exactly the opposite — and not only because women have to hold their keys in self-defense while walking through parking lots at night. It’s only very recently that women have widely participated in public life, but it’s even more recently that men have been welcome, or even expected, to provide physical care for vulnerable people.\n",
      "Only for a nanosecond of human history have men even slightly shared what was once exclusively a woman’s burden: the relentless daily labor of caring for another person’s body, the life-preserving work of cleaning feces and vomit, the constant cycle of cooking and feeding and blanketing and bathing, whether for the young, the ill or the old. For nearly as long as there have been humans, being a female human has meant a daily nonoptional immersion in the fragility of human life and the endless effort required to sustain it.\n",
      "Obviously not everyone who provides care for others is a saint. But engaging in that daily devotion, or even living with its expectation, has enormous potential to change a person. It forces one to constantly imagine the world from someone else’s point of view: Is he hungry? Maybe she’s tired. Is his back hurting him? What is she trying to say?\n",
      "The most obvious cure for today’s gender inequities is to put more women in power. But if we really hope to create an equal society, we will also need more men to care for the powerless — more women in the boardroom, but also more men at the nurses’ station and the changing table, immersed in daily physical empathy. If that sounds like an evolutionary impossibility, well, it doesn’t violate the laws of physics, so we can achieve it. It is surely worth at least as much investment as defeating death.\n",
      "Perhaps it takes the promise of immortality to inspire the self-absorbed to invest in unsexy work like Alzheimer’s research. If so, we may all one day bless the inane death-defiance as a means to a worthy end.\n",
      "But men who hope to live forever might pause on their eternal journey to consider the frightening void at invincibility’s core. Death is the ultimate vulnerability. It is the moment when all of us must confront exactly what so many women have known all too well: You are a body, only a body, and nothing more.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "class Content: # Each data as a class object. \n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "\n",
    "def getPage(url):\n",
    "    req = requests.get(url)\n",
    "    return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "\n",
    "def scrapeNYTimes(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find('h1').text\n",
    "    lines = bs.select('div.StoryBodyCompanionColumn div p')\n",
    "    body = '\\n'.join([line.text for line in lines])\n",
    "    return Content(url, title, body)\n",
    "\n",
    "def scrapeBrookings(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find('h1').text\n",
    "    body = bs.find('div', {'class', 'post-body'}).text\n",
    "    return Content(url, title, body)\n",
    "\n",
    "\n",
    "url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\n",
    "content = scrapeBrookings(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}\\n'.format(content.url))\n",
    "print(content.body)\n",
    "\n",
    "url = 'https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'\n",
    "content = scrapeNYTimes(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}\\n'.format(content.url))\n",
    "print(content.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things even more convenient, rather than dealing with all of these tag arguments and key/value pairs, you can use the BeautifulSoup select function with a single string CSS selector for each piece of information you want to collect and put all of these selectors in a dictionary object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"\n",
    "    Common base class for all articles/pages\n",
    "    \"\"\"\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function controls output\n",
    "        \"\"\"\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))\n",
    "\n",
    "class Website:\n",
    "    \"\"\" \n",
    "    Contains information about website structure\n",
    "    \n",
    "    Note that the Website class does not store information collected from the individual pages themselves, but stores instructions about how to collect that data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "\n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def safeGet(self, pageObj, selector):\n",
    "        \"\"\"\n",
    "        Utilty function used to get a content string from a Beautiful Soup\n",
    "        object and a selector. Returns an empty string if no object\n",
    "        is found for the given selector\n",
    "        \"\"\"\n",
    "        selectedElems = pageObj.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def parse(self, site, url):\n",
    "        \"\"\"\n",
    "        Extract content from a given page URL\n",
    "        \"\"\"\n",
    "        bs = self.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = self.safeGet(bs, site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(url, title, body)\n",
    "                content.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: http://shop.oreilly.com/product/0636920028154.do\n",
      "TITLE: Learning Python, 5th Edition \n",
      "BODY:\n",
      "\n",
      "Get a comprehensive, in-depth introduction to the core Python language with this hands-on book. Based on author Mark Lutz’s popular training course, this updated fifth edition will help you quickly write efficient, high-quality code with Python. It’s an ideal way to begin, whether you’re new to programming or a professional developer versed in other languages. \n",
      "\n",
      "Complete with quizzes, exercises, and helpful illustrations,  this easy-to-follow, self-paced tutorial gets you started with both Python 2.7 and 3.3— the latest releases in the 3.X  and 2.X lines—plus all other releases in common use today. You’ll also learn some advanced language features that recently have become more common in Python code.\n",
      "\n",
      "Explore Python’s major built-in object types such as numbers, lists, and dictionaries \n",
      "Create and process objects with Python statements, and learn Python’s general syntax model\n",
      "Use functions to avoid code redundancy and package code for reuse\n",
      "Organize statements, functions, and other tools into larger components with modules \n",
      "Dive into classes: Python’s object-oriented programming tool for structuring code\n",
      "Write large programs with Python’s exception-handling model and development tools\n",
      "Learn advanced Python tools, including decorators, descriptors, metaclasses, and Unicode processing\n",
      "\n",
      "\n",
      "URL: https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/\n",
      "TITLE: Idea to Retire: Old methods of policy education\n",
      "Idea to Retire: Old methods of policy education\n",
      "BODY:\n",
      "\n",
      "Public policy and public affairs schools aim to train competent creators and implementers of government policy. While drawing on the principles that gird our economic and political systems to provide a well-rounded education, like law schools and business schools, policy schools provide professional training. They are quite distinct from graduate programs in political science or economics which aim to train the next generation of academics. As professional training programs, they add value by imparting both the skills which are relevant to current employers, and skills which we know will be relevant as organizations and societies evolve. \n",
      "The relevance of the skills that policy programs impart to address problems of today and tomorrow bears further discussion. We are living through an era in which societies are increasingly interconnected. The wide-scale adoption of devices such as the smartphone is having a profound impact on our culture, communities, and economy. The use of social and digital media and associated means of communication enabled by mobile devices is changing the tone, content, and geographic scope of our conversations, modifying how information is generated and consumed, and changing the very nature of citizen engagement. \n",
      "Information technology-based platforms provisioned by private providers such as Facebook, Google, Uber, and Lyft maintain information about millions of citizens and enable services such as transportation that were mediated in the past solely by the public sector. Surveillance for purposes of public safety via large-scale deployment of sensors also raises fundamental questions about information privacy. From technology-enabled global delivery of work to displacement and replacement of categories of work, some studies estimate that up to 47 percent of U.S. employment might be at risk of computerization with an attendant rise in income inequality. These technology-induced changes will affect every policy domain. How should policy programs best prepare students to address societal challenges in this world that is being transformed by technology? We believe the answer lies in educating students to be “men and women of intelligent action.” \n",
      "A model of policy education\n",
      "We begin with a skills-based model of policy education. These four essential skills address the general problems policy practitioners frequently face:\n",
      "\n",
      "Design skills to craft policy ideas \n",
      "Analytical skills to make smart ex ante decisions \n",
      "Interpersonal experience to manage policy implementation  \n",
      "Evaluative skills to assess outcomes ex post and correct course if necessary\n",
      "\n",
      "These skills make up the policy analysis toolkit required to be data driven practitioner of “intelligent action” in any policy domain. This toolkit needs to be supplemented by an understanding of how technology is transforming societal challenges, enabling new solutions, or disrupting existing regulatory regimes. This understanding is essential to policy formulation and implementation. \n",
      "Pillar 1: Design skills\n",
      "As with engineering, where design precedes analysis, this first pillar seeks to educate students in thinking creatively about problems in order to devise and develop policy ideas. Using ideas derived from design, divergent and convergent thinking principles are employed to generate, explore, and arrive at a candidate set of solutions. Using Uber as an example, an approach to identify and explore the key policy issues such as convenience, costs, driver working hours, and insurance would involve interviewing and observing both incumbent taxi drivers and Uber drivers. This in turn would lead to a set of alternatives that deserve further and careful consideration.  Using these skills, candidate designs and choices that are generated can be evaluated using the policy analytic toolkit. \n",
      "Pillar 2: Analytical skills\n",
      "At Carnegie Mellon, we are often cited in media and interrogated by peers on our approach to analytical and technology skills education. Curiosity about which skills are the “right” skills to teach policy practitioners are common, but we believe this is the wrong approach. We instead begin from the premise that policy or management decisions should be grounded in evidence.  We then determine the skills required to assemble the types of evidence that will likely be available to policy makers in the future.  In increasingly instrumented environments where citizens and infrastructure produce continuous streams of data, making sense of it all will require a somewhat different set of skills. We believe that a grounding in micro-economics, operations research, statistics, and program evaluation (aka causal inference) to be an essential core to policy programs. \n",
      "New coursework will teach students to work with multi-variable data and machine learning with an emphasis on prediction. This material ought to be part of the required coursework in statistics given the importance of prediction in many policy implementation settings. Along the same lines, the ability to work with unstructured data (especially text) and data visualization will become increasingly relevant to all students, not just those students who want to specialize in data analytics. Finally, knowledge of data manipulation and analysis languages such as Python and R for analytic work will be important because data often has to be massaged and cleansed prior to analysis. An important task for programs will be to determine the competencies expected of graduates. \n",
      "Pillar 3: Interpersonal experiences\n",
      "The third pillar of the skills-based model is interpersonal experience, where the practiced habits of good communication and steady negotiation developed with a sound understanding of organizations, their design and their behaviors. We label these purposely as experiences rather than skills because we believe they are best practiced either in the real-world or in simulated real-world settings. It is also in this pillar where practitioners learn the knowledge necessary to become credible experts in their domain. We believe that in addition to core coursework in the area, a supplementary curriculum which provides students with opportunities to gain these experiences is an essential component of our educational model.\n",
      "Pillar 4: Evaluative skills\n",
      "\n",
      "\n",
      "Related Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Constitution 3.0\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tEdited by Jeffrey Rosen and Benjamin Wittes \n",
      "2013\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "The Need for Speed\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tBy Robert E. Litan and Hal J. Singer \n",
      "2013\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Talk is Cheap\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tBy Robert W. Crandall and Leonard Waverman \n",
      "2010\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The ability to carefully diagnose the effectiveness of policy or management interventions is the fourth pillar of our model. It is insufficient to create and execute policy without measurement, and this is where both careful thought to the fundamental issues of measurement and evaluation become important. The ability to make objective judgments on the benefits, liabilities, and unintended consequences of prior policies is the goal of this set of skills. Here, sound statistical and econometric training with an understanding of the principles of causal inference is essential. In addition, program evaluation skills such as cost-benefit and financial analysis help practitioners round out their evaluation skills by considering both non-monetary and economic impacts.\n",
      "What should be retired?\n",
      "A skills-based approach might replace certain aspects of existing policy training.  This depends on a number of factors specific to each institution, but three generally applicable observations are clear. First, real-world experiences are a powerful way to encode domain learning as well as project management skills. Through project-based work, students can learn about institutional contexts in specific policy domains and political processes such as budgeting. Second, team-based projects allow students to learn and apply principles of management and organizational behavior. At Carnegie Mellon, we refer to these as “systems synthesis” projects, since they require students to adopt a systemic point of view and to synthesize a number of skills in their policy analysis toolkit. Third, interpersonal skills training can be practiced through activities such as weekend negotiation exercises, hackathons, and speaker series. These activities can be highly intentional and fashioned to reinforce skills rather than as a recess from the “real work” of classroom training. Since students complete graduate programs in such a short time, counseling them to focus on outcomes from day one will allow them to choose a reinforcing set of coursework and real-world experiences. \n",
      "In summary, we argue for a model of policy education that views practitioners as future problem solvers. Good policy education must consider the ways in which problems will present themselves, and the ways in which answers will obscure themselves. Rigorous training grounded in the analysis of available evidence and buoyed by real-world interpersonal experiences is a sound approach to relevant, durable policy training.\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "R\n",
      "\n",
      "\n",
      "\n",
      "Ramayya Krishnan\n",
      "Ramayya Krishnan is the dean of H. John Heinz III College of Information Systems and Public Policy at Carnegie Mellon University where he is the W.W. Cooper and Ruth F. Cooper Professor of Management Science and Information Systems.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "J\n",
      "\n",
      "\n",
      "\n",
      "Jon Nehlsen\n",
      "Jon Nehlsen is senior director of external relations at H. John Heinz III College of Information Systems and Public Policy at Carnegie Mellon University.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Read other essays in the Ideas to Retire blog series here.\n",
      "\n",
      "URL: https://www.nytimes.com/2018/01/28/business/energy-environment/oil-boom.html\n",
      "TITLE: Oil Boom Gives the U.S. a New Edge in Energy and Diplomacy\n",
      "BODY:\n",
      "HOUSTON — A substantial rise in oil prices in recent months has led to a resurgence in American oil production, enabling the country to challenge the dominance of Saudi Arabia and dampen price pressures at the pump.\n",
      "The success has come in the face of efforts by Saudi Arabia and its oil allies to undercut the shale drilling spree in the United States. Those strategies backfired and ultimately ended up benefiting the oil industry.\n",
      "Overcoming three years of slumping prices proved the resiliency of the shale boom. Energy companies and their financial backers were able to weather market turmoil — and the maneuvers of the global oil cartel — by adjusting exploration and extraction techniques.\n",
      "After a painful shakeout in the industry that included scores of bankruptcies and a significant loss of jobs, a steadier shale-drilling industry is arising, anchored by better-financed companies.\n",
      "With the price of West Texas intermediate crude above $65 a barrel, a level not seen in almost three years, the United States is becoming a dominant producer. It is able to outflank competitors in supplying growing global markets, particularly China and India, while slashing imports from the Middle East and North Africa.\n",
      "This year, the United States is expected to surpass Saudi Arabia and to rival Russia as the world’s leader, with record output of over 10 million barrels a day, according to the International Energy Agency.\n",
      "“This is a 180-degree turn for the United States and the impacts are being felt around the world,” said Daniel Yergin, the economic historian and author of “The Prize: The Epic Quest for Oil, Money and Power.” “This not only contributes to U.S. energy security but also contributes to world energy security by bringing new supplies to the world.”\n",
      "At the same time, the United States is becoming a major exporter of natural gas, another outgrowth of the shale revolution, undercutting Russian energy dominance over Eastern Europe.\n",
      "The improving energy picture comes as the Trump administration is attempting to increase offshore drilling and loosen other regulations on fossil fuel development. But just as the surge in oil and gas production in shale fields during the Barack Obama administration had little to do with Washington, the current rise is the result of private companies responding to global markets.\n",
      "Shale fields can be developed relatively quickly and at modest costs relative to the giant projects, whether on land or offshore, that were once favored by big oil companies. That makes it easier to turn investment spigots on or off to adjust to market fluctuations. Companies like Exxon Mobil and Chevron are putting increasing amounts of capital in shale fields, particularly in West Texas and New Mexico.\n",
      "The results go far beyond the economic, offering Washington strategic weapons once unthinkable. The United States and its allies now have a supply cushion at a time when political turmoil in Venezuela, Libya and Nigeria is threatening to interrupt flows to markets.\n",
      "Only a few years ago, such threats — along with a recent pipeline failure in the North Sea and storms in the Gulf of Mexico — would have sent the price of crude soaring. Instead, the rise has been muted, and gasoline at the pump remains below $2.60 a gallon across most of the United States.\n",
      "The new energy power also relieves pressure on Washington to act militarily if tensions between Iran and Saudi Arabia break out into war. And it gives Washington the leeway to apply sanctions on other producers — as it has in Russia, and may in Iran or Venezuela — with far less risk to the global economy.\n",
      "It is a striking contrast to the 1970s, when Arab oil boycotts forced motorists to line up for blocks to fill their tanks and the economy went into a tailspin. Even more recently, during the presidency of George W. Bush, domestic oil output was declining so rapidly that the country set a course to replace oil with biofuels like ethanol.\n",
      "Many environmentalists argue that by increasing oil and gas supplies and lowering prices for consumers, shale drilling is extending the life of fossil fuels to the detriment of the environment and the development of cleaner energy.\n",
      "The shale drilling revolution has remade the global energy market, with imports from members of the Organization of the Petroleum Exporting Countries plunging by 20 percent from late 2016 to late 2017. At the same time, exports rose by hundreds of thousands of barrels a day.\n",
      "Nothing like the current situation was foreseen in late 2014, when rising domestic production began weighing on global oil prices.\n",
      "In response, Saudi Arabia led OPEC in a new direction. Instead of throttling back to support prices as the cartel had done so often, it left the market alone and even increased production for a time.\n",
      "Prices fell below $40 a barrel, as the Saudis and their allies hoped to drive American operations out of business by making shale drilling uneconomical. American exploration quickly dropped, but the price squeeze made companies more innovative in the use of drilling technologies, robotics and sensors to maximize output and reduce costs.\n",
      "While scores of smaller companies went out of business, the survivors lengthened horizontal wells to yield more oil, and used clever hedging and drilling strategies to maximize profits even when prices slumped.\n",
      "The response surprised the global oil community. OPEC, Russia and allied producing countries changed course and began cutting back again in 2016.\n",
      "“OPEC missed the point,” said René Ortiz, a former OPEC secretary general and former Ecuadorean energy minister. “They thought they could recover the U.S. market by bringing the prices down. Now the U.S. has gained the leading position in the world oil market regardless of what OPEC does.”\n",
      "“This displacement of Saudi oil, Nigerian oil, Libyan oil and Venezuelan oil,” Mr. Ortiz concluded, “was never anticipated.”\n",
      "A week ago, OPEC leaders met in Oman to discuss a probable extension of production cuts into 2019 to support prices. Their biggest obstacle is the United States.\n",
      "Technological advances unlocking oil from tight rocks like shale has led to a drilling frenzy enabling a doubling of output in a decade, transforming unlikely places like North Dakota and New Mexico into world class petroleum hubs. Pipelines are being built across Texas to serve ports where oil can be pumped onto tankers headed for China, India and other markets.\n",
      "Domestic production last year averaged 9.3 million barrels a day, and the Energy Department projects that the figure will climb to 10.3 million barrels a day this year, surpassing the record set in 1970. In the meantime, since a 40-year export ban was lifted in 2015, exports of American oil have risen to roughly two million barrels a day — more than many OPEC members.\n",
      "The department projects an additional increase in domestic production of 500,000 barrels a day in 2019.\n",
      "Concerns over climate change as well as the growing popularity of electric cars and the eventual aging of the best shale fields will probably curb production and demand over the next few decades. But in the short term, the boom has changed the landscape.\n",
      "The Energy Department projects that the recent surge will hold the price of Brent crude, the global benchmark, to $60 a barrel in 2018 and $61 a barrel in 2019 — a modest increase from $54 last year. (The Brent price rose above $70 a barrel this month, but few analysts see a return to $100-a-barrel oil.)\n",
      "The emerging order in the energy realm is a stable balance of power. Saudi Arabia, which essentially runs OPEC, has put a floor under the oil price — probably around $50 a barrel — with its limits on output and exports over the last four years. But now the United States, by the sheer force of its production, the supremacy of its technology, and an unmatched pipeline, refinery and storage structure, has put a ceiling to the price.\n",
      "Experts note that when oil climbs to $60 a barrel and higher, as it has lately, a drilling rush commences — the national rig count has climbed by over a third in the last year — promising to refill domestic and even global energy inventories. Only a major war or other disruption is likely to send prices soaring.\n",
      "“We have all suffered these depressed prices over the last two years and we are excited to see the new prices and we will respond accordingly,” said Harald Jordan, vice president for engineering at Peak Energy, a Colorado-based producer. “You will see rig activity continue to increase.”\n"
     ]
    }
   ],
   "source": [
    "# This is way more scalable. \n",
    "\n",
    "crawler = Crawler()\n",
    "\n",
    "siteData = [\n",
    "    ['O\\'Reilly Media', 'http://oreilly.com', 'h1', 'section#product-description'],\n",
    "    ['Reuters', 'http://reuters.com', 'h1', 'div.StandardArticleBody_body_1gnLA'],\n",
    "    ['Brookings', 'http://www.brookings.edu', 'h1', 'div.post-body'],\n",
    "    ['New York Times', 'http://nytimes.com', 'h1', 'div.StoryBodyCompanionColumn div p']\n",
    "]\n",
    "websites = []\n",
    "for row in siteData:\n",
    "    websites.append(Website(row[0], row[1], row[2], row[3]))\n",
    "\n",
    "crawler.parse(websites[0], 'http://shop.oreilly.com/product/0636920028154.do')\n",
    "crawler.parse(\n",
    "    websites[1], 'http://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0')\n",
    "crawler.parse(\n",
    "    websites[2],\n",
    "    'https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/')\n",
    "crawler.parse(\n",
    "    websites[3], \n",
    "    'https://www.nytimes.com/2018/01/28/business/energy-environment/oil-boom.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring Crawlers\n",
    "\n",
    "Creating flexible and modifiable website layout types doesn’t do much good if you still have to locate each link you want to scrape by hand. The previous chapter showed various methods of crawling through websites and finding new pages in an automated way.\n",
    "\n",
    "This section shows how to incorporate these methods into a well-structured and expandable website crawler that can gather links and discover data in an automated way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling through sites with search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"Common base class for all articles/pages\"\"\"\n",
    "\n",
    "    def __init__(self, topic, url, title, body):\n",
    "        self.topic = topic\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        self.url = url\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function controls output\n",
    "        \"\"\"\n",
    "        print('New article found for topic: {}'.format(self.topic))\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    \"\"\"Contains information about website structure\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.searchUrl = searchUrl\n",
    "        self.resultListing = resultListing\n",
    "        self.resultUrl = resultUrl\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code loops through all topics and then loops through all websites in the inner loop. --> Less load placed on any one web server. Distributing requests is generally better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GETTING INFO ABOUT: python\n",
      "New article found for topic: python\n",
      "URL: Learning Python, 5th Edition \n",
      "TITLE: \n",
      "Get a comprehensive, in-depth introduction to the core Python language with this hands-on book. Based on author Mark Lutz’s popular training course, this updated fifth edition will help you quickly write efficient, high-quality code with Python. It’s an ideal way to begin, whether you’re new to programming or a professional developer versed in other languages. \n",
      "\n",
      "Complete with quizzes, exercises, and helpful illustrations,  this easy-to-follow, self-paced tutorial gets you started with both Python 2.7 and 3.3— the latest releases in the 3.X  and 2.X lines—plus all other releases in common use today. You’ll also learn some advanced language features that recently have become more common in Python code.\n",
      "\n",
      "Explore Python’s major built-in object types such as numbers, lists, and dictionaries \n",
      "Create and process objects with Python statements, and learn Python’s general syntax model\n",
      "Use functions to avoid code redundancy and package code for reuse\n",
      "Organize statements, functions, and other tools into larger components with modules \n",
      "Dive into classes: Python’s object-oriented programming tool for structuring code\n",
      "Write large programs with Python’s exception-handling model and development tools\n",
      "Learn advanced Python tools, including decorators, descriptors, metaclasses, and Unicode processing\n",
      "\n",
      "\n",
      "BODY:\n",
      "http://shop.oreilly.com/product/0636920028154.do\n",
      "New article found for topic: python\n",
      "URL: Introducing Python, 2nd Edition \n",
      "TITLE: \n",
      "Easy to understand and fun to read, this updated edition of Introducing Python is ideal for beginning programmers as well as those new to the language. Author Bill Lubanovic takes you from the basics to more involved and varied topics, mixing tutorials with cookbook-style code recipes to explain concepts in Python 3. End-of-chapter exercises help you practice what you’ve learned.\n",
      "\n",
      "You’ll gain a strong foundation in the language, including best practices for testing, debugging, code reuse, and other development tips. This book also shows you how to use Python for applications in business, science, and the arts, using various Python tools and open source packages.\n",
      "\t\t\n",
      "BODY:\n",
      "http://shop.oreilly.com/product/0636920252528.do\n",
      "New article found for topic: python\n",
      "URL: Python for DevOps\n",
      "TITLE: \n",
      "Much has changed in technology over the past decade. Data is hot, the cloud is ubiquitous, and many organizations need some form of automation. Throughout these transformations, Python has become one of the most popular languages in the world. This practical resource shows you how to use Python for everyday Linux systems administration tasks with today’s most useful DevOps tools, including Docker, Kubernetes, and Terraform.\n",
      "\n",
      "Learning how to interact and automate with Linux is essential for millions of professionals. Python makes it much easier. With this book, you’ll learn how to develop software and solve problems using containers, as well as how to monitor, instrument, load-test, and operationalize your software. Looking for effective ways to \"get stuff done\" in Python? This is your guide.\n",
      "\n",
      "Python foundations, including a brief introduction to the language\n",
      "How to automate text, write command-line tools, and automate the filesystem\n",
      "Linux utilities, package management, build systems, monitoring and instrumentation, and automated testing\n",
      "Cloud computing, infrastructure as code, Kubernetes, and serverless\n",
      "Machine learning operations and data engineering from a DevOps perspective\n",
      "Building, deploying, and operationalizing a machine learning project\n",
      "\n",
      "\n",
      "BODY:\n",
      "http://shop.oreilly.com/product/0636920274902.do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-10-2cc6e9570188>\", line 61, in <module>\n",
      "    crawler.search(topic, targetSite)\n",
      "  File \"<ipython-input-10-2cc6e9570188>\", line 31, in search\n",
      "    bs = self.getPage(site.url + url)\n",
      "  File \"<ipython-input-10-2cc6e9570188>\", line 8, in getPage\n",
      "    req = requests.get(url)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\requests\\api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\requests\\api.py\", line 60, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\requests\\sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\requests\\sessions.py\", line 668, in send\n",
      "    history = [resp for resp in gen] if allow_redirects else []\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\requests\\sessions.py\", line 668, in <listcomp>\n",
      "    history = [resp for resp in gen] if allow_redirects else []\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\requests\\sessions.py\", line 247, in resolve_redirects\n",
      "    **adapter_kwargs\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\requests\\sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 421, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 416, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\http\\client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\http\\client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\http\\client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\OpenSSL\\SSL.py\", line 1839, in recv_into\n",
      "    result = _lib.SSL_read(self._ssl, buf, nbytes)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\chlje\\Anaconda3\\envs\\web_scraping37\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Crawler:\n",
    "\n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def safeGet(self, pageObj, selector):\n",
    "        childObj = pageObj.select(selector)\n",
    "        if childObj is not None and len(childObj) > 0:\n",
    "            return childObj[0].get_text()\n",
    "        return ''\n",
    "\n",
    "    def search(self, topic, site):\n",
    "        \"\"\"\n",
    "        Searches a given website for a given topic and records all pages found\n",
    "        \"\"\"\n",
    "        bs = self.getPage(site.searchUrl + topic)\n",
    "        searchResults = bs.select(site.resultListing)\n",
    "        for result in searchResults:\n",
    "            url = result.select(site.resultUrl)[0].attrs['href']\n",
    "            # Check to see whether it's a relative or an absolute URL\n",
    "            if(site.absoluteUrl):\n",
    "                bs = self.getPage(url)\n",
    "            else:\n",
    "                bs = self.getPage(site.url + url)\n",
    "            if bs is None:\n",
    "                print('Something was wrong with that page or URL. Skipping!')\n",
    "                return\n",
    "            title = self.safeGet(bs, site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(topic, title, body, url)\n",
    "                content.print()\n",
    "\n",
    "\n",
    "crawler = Crawler()\n",
    "\n",
    "siteData = [\n",
    "    ['O\\'Reilly Media', 'http://oreilly.com', 'https://ssearch.oreilly.com/?q=',\n",
    "        'article.product-result', 'p.title a', True, 'h1', 'section#product-description'],\n",
    "    ['Reuters', 'http://reuters.com', 'http://www.reuters.com/search/news?blob=', 'div.search-result-content',\n",
    "        'h3.search-result-title a', False, 'h1', 'div.StandardArticleBody_body_1gnLA'],\n",
    "    ['Brookings', 'http://www.brookings.edu', 'https://www.brookings.edu/search/?s=',\n",
    "        'div.list-content article', 'h4.title a', True, 'h1', 'div.post-body']\n",
    "]\n",
    "sites = []\n",
    "for row in siteData:\n",
    "    sites.append(Website(row[0], row[1], row[2],\n",
    "                         row[3], row[4], row[5], row[6], row[7]))\n",
    "\n",
    "topics = ['python', 'data science']\n",
    "for topic in topics:\n",
    "    print('GETTING INFO ABOUT: ' + topic)\n",
    "    for targetSite in sites:\n",
    "        crawler.search(topic, targetSite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling Sites through Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "\n",
    "    def __init__(self, name, url, targetPattern, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.targetPattern = targetPattern\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "\n",
    "\n",
    "class Content:\n",
    "\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, site):\n",
    "        self.site = site\n",
    "        self.visited = []\n",
    "\n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def safeGet(self, pageObj, selector):\n",
    "        selectedElems = pageObj.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def parse(self, url):\n",
    "        bs = self.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = self.safeGet(bs, self.site.titleTag)\n",
    "            body = self.safeGet(bs, self.site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(url, title, body)\n",
    "                content.print()\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "        Get pages from website home page\n",
    "        \"\"\"\n",
    "        bs = self.getPage(self.site.url)\n",
    "        targetPages = bs.findAll('a', href=re.compile(self.site.targetPattern))\n",
    "        for targetPage in targetPages:\n",
    "            targetPage = targetPage.attrs['href']\n",
    "            if targetPage not in self.visited:\n",
    "                self.visited.append(targetPage)\n",
    "                if not self.site.absoluteUrl:\n",
    "                    targetPage = '{}{}'.format(self.site.url, targetPage)\n",
    "                self.parse(targetPage)\n",
    "\n",
    "\n",
    "reuters = Website('Reuters', 'https://www.reuters.com', '^(/article/)',\n",
    "                  False, 'h1', 'div.StandardArticleBody_body_1gnLA')\n",
    "crawler = Crawler(reuters)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling multiple page types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike crawling through a predetermined set of pages, crawling through all internal links on a website can present a challenge in that you never know exactly what you’re getting. Fortunately, there are a few basic ways to identify the page type:\n",
    "- By URL pattern. e.g) ../posts/title-of-post \n",
    "- By the presence or lack of certain field on the site. e.g) author name, product image etc. \n",
    "- By the presence of certain tags on the page to identify the page. \n",
    "\n",
    "You can use page_type attribute, for example, to inherit Website class to create Product or Article classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    \"\"\"Common base class for all articles/pages\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product(Website):\n",
    "    \"\"\"Contains information for scraping a product page\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, productNumber, price):\n",
    "        Website.__init__(self, name, url, TitleTag)\n",
    "        self.productNumberTag = productNumberTag\n",
    "        self.priceTag = priceTag\n",
    "\n",
    "class Article(Website):\n",
    "    \"\"\"Contains information for scraping an article page\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, bodyTag, dateTag):\n",
    "        Website.__init__(self, name, url, titleTag)\n",
    "        self.bodyTag = bodyTag\n",
    "        self.dateTag = dateTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parsePage(url):\n",
    "    \n",
    "    if '/ideas/' in url:\n",
    "        \n",
    "\n",
    "oreilly = Website('O\\'Reilly', 'https://oreilly.com', 'h1' '')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking about web crawler models\n",
    "\n",
    "You have to think first. Don't drink information from a fire hose. Make clear what you need and how you need it. \n",
    "\n",
    "When collecting similar data across multiple domains or from multiple sources, always try to normalize them. Dealing with data with identical and comparable fields is much easier than dealing with data that is completely dependent on the format of its original source. (Minimize preprocessing)\n",
    "\n",
    "Always build scrapers under the assumption that more sources of data will be added to them in the future. Minimize the programming overhead required to add these new sources. Find the optimal underlying patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
